input {
 # beat에서 데이터를 받을 logstash의 port지정
  beats {
    port => 5044  # filebeat.yml 191번째와 포트번호 맞춰서 연결시켜줌
  }
}

filter {
  mutate {
    # 실제 데이터는 "message" 필드로 오기 때문에 csv형태의 내용을 분할하여 새로운 이름으로 필드를 추가 
    split => [ "message",  "," ] 
    add_field => {
      "date" => "%{[message][0]}"
      "confirmed" => "%{[message][1]}"
      "death" => "%{[message][2]}"
      "released" => "%{[message][3]}"
      "revenue" => "%{[message][4]}"
      "revenue_growth" => "%{[message][5]}"
      "audience" => "%{[message][6]}"
      "audience_growth" => "%{[message][7]}"
      "confirmed_growth" => "%{[message][8]}"
      "QoQ_sub" => "%{[message][9]}"
    }

    # 기본으로 전송되는 데이터 분석에 불필요한 필드 제거
    remove_field => ["ecs", "host", "@version", "agent", "log", "tags",  "input", "message"]
  }

  # "date" 필드를 이용하여 Elasticsearch에서 인식할 수 있는 date 타입의 형태로 필드를 추가
  date {
    match => [ "date", "yyyy_ww"]
    timezone => "America/Montreal"
    locale => "en"
    target => "convert_date" # 년월일 표기 타입
  }

  # Kibana에서 데이터 분석시 필요하기 때문에 숫자 타입으로 변경
  mutate {
    convert => {
      "confirmed" => "float"
      "death" => "float"
      "released" => "float"
      "revenue" => "float"
      "revenue_growth" => "float"
      "audience" => "float"
      "audience_growth" => "float"
      "confirmed_growth" => "float"
      "QoQ_sub" => "float"
    }
    remove_field => [ "@timestamp" ]
  }
}

output {

  # 콘솔창에 어떤 데이터들로 필터링 되었는지 확인
  stdout {
          codec => rubydebug
  }

  # 위에서 설치한 Elasticsearch 로 "covid-19" 라는 이름으로 인덱싱 
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "net_movie_covid19"
  }
}